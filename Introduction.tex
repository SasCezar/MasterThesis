\chapter{Introduction}
\paragraph{}

With the advent of the internet, a large amount of digital text is produced each day as news articles, research publications, blogs, and social media. It is essential to advance techniques for automatically extracting information from these documents, as much valuable knowledge is hidden within them. This extracted information can be used to enhance access and management of knowledge hidden in large text corpora. The computer science field that concerns the processing of natural language is called Natural Language Processing (NLP). 

However, most of this information is produced only in English; indeed, it is a widely lamented fact that linguistic and encyclopedic resources are heavily biased towards English. Even multilingual knowledge bases (KBs) such as Wikidata \citep{vwikidata} are predominantly English-based \citep{Kaffee:Simperl:18}. This suggests that coverage is higher for English and that facts of interest to English-speaking communities are more likely included in a KB.

This also means that current natural language processing research and development has mainly focused on English. Recognizing this, the community has in recent years put emphasis on multilingual and cross-lingual resources and methods which allow for the joint exploitation of resources between languages and for the transfer of models from well-resourced languages to lower-resourced ones.

In this work, we discuss such problems in the context of Relation Extraction (RE), which is an Information Extraction (IE) subtask. IE is a field that concerns NLP, computational linguistic, and data mining. The goal of IE is to extract structured information from a collection of unstructured documents, this is achieved via different subtask like named entity recognition, coreference resolution, and relation extraction. Relation extraction consists of identifying mentions of the relations of interest in each sentence of the given documents, the extracted relation are then used to populate knowledge bases. Traditional relation extraction system are required to only extract seen relations, which make easier to train these models as the data required can be obtained via crowd sourcing~\citep{liu-etal-2016-effective} or distant supervision~\citep{hoffmann-etal-2011-knowledge}. These models are meant to generalize to new entities, but {\em not} new {\em relations}. In this work, we consider the  case in which models are trained on a subset of pre-specified relations and applied to both seen and unseen entities, and unseen relations. The latter scenario is known as {\em zero-shot} RE \citep{rocktaschel2015injecting}.

As a possible solution to this problem,~\cite{levy2017zero} propose an alternative approach to the relation extraction task. By reframing the problem as question answering, they can use reading comprehension techniques that allow zero-shot relation extraction. 

Our interest in multilingual relation extraction concerns three different questions: \begin{enumerate}[a) , font=\bfseries, noitemsep]
    \item how well relation extraction models can be transferred across different languages?
    \item can the variance in the number of relations examples between languages help  build more robust relation extraction models?
    \item can a jointly trained model that performs RE on multiple languages perform equally or better than the monolingual trained version?
\end{enumerate}

Unfortunately, as aforementioned, there is a lack of multilingual resources, and this is also true for  relation extraction. For the best of our knowledge, there is no large corpora for multilingual relation extraction available. The only resource contains around 90K examples and it is available only in three languages. To solve this and answer our questions, based on~\citep{levy2017zero, hewlett2016wikireading} we propose \textbf{X-WikiRE}, a new multilingual resource for relation extraction as question answering and using state-of-the-art models we investigate how such resource can be used in different transfer learning scenarios. By performing various experiments we show how our multilingual resource helps both in case of low resource languages by performing sequential transfer learning, and also that \textbf{X-WikiRE} improves the performances on single languages when jointly training a single model on multiple languages.



% This work explores techniques for automatically filling such language gaps by learning to add facts in other languages, and more generally shows that multilingual sharing is beneficial for knowledge base completion. 


\paragraph{}
This work is organized through the following chapters: in Chapter~\ref{chpt:2} we present some natural language processing and machine learning concepts and methodologies background. In Chapter~\ref{chpt:3} we present the state of the art in relation extraction and machine comprehension. Next, Chapter~\ref{chpt:4} presents \textbf{X-WikiRE}, our new multilingual relation extraction dataset; while in Chapter~\ref{chpt:5} we describe the used machine comprehension model and our experimental setup. Finally, in Chapter~\ref{chpt:6} we present the results obtained using our dataset. To conclude, Chapter~\ref{chpt:7} contains some final thoughts and future developments.